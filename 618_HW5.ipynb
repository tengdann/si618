{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUfEjrnxfqbP"
   },
   "source": [
    "# SI 618 Homework 5: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HiF5779SXEsY"
   },
   "source": [
    "## The total score for this assignment will be 100 points, consisting of:\n",
    "- 10 pt: Overall quality of spelling, grammar, puctuation, etc. of written sentences.\n",
    "- 10 pt: Codes are written in [PEP 8](https://www.python.org/dev/peps/pep-0008/) style.\n",
    "- 80 pt: Homework questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ri4qlLSPL5W"
   },
   "outputs": [],
   "source": [
    "MY_UNIQNAME = 'tengdann'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZAtMMYfPdPJ"
   },
   "source": [
    "# Download the data:\n",
    "We will be using a subset of the data from [Project Gutenberg](http://www.gutenberg.org).\n",
    "\n",
    ">Project Gutenberg is a volunteer effort to digitize and archive cultural works, to \"encourage the creation and distribution of eBooks\". It was founded in 1971 by American writer Michael S. Hart and is the oldest digital library. Most of the items in its collection are the full texts of public domain books. The project tries to make these as free as possible, in long-lasting, open formats that can be used on almost any computer., Project Gutenberg reached 57,000 items in its collection of free eBooks.\n",
    "\n",
    "We will use a [cleaned up corpus](https://github.com/aparrish/gutenberg-dammit) that was processed by [Allison Parish](https://www.decontextualize.com/). This processing step was done to standardize metadata and text encoding. The dataset is described on the [project page](https://github.com/aparrish/gutenberg-dammit#gutenberg-dammit). Note that there are plain text files withing subdirectories numbered with the first three digits of the document ID, and there is also a JSON file of document metadata with title, author, and other information.\n",
    "\n",
    "\n",
    "We created a reduced subset of 4000 documents for the purpose of this assignment that can be downloaded below:\n",
    "\n",
    "https://drive.google.com/file/d/1V5ep_5veAccCD-YxGlVfv3DsXczBIvv5/view?usp=sharing\n",
    "\n",
    "This ZIP file is approximately 550 MB compressed, and 1.45 GB uncompressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmyEphdRPjQT"
   },
   "source": [
    "## Q1. (10 pts.) Data cleaning\n",
    "- (3 pts.) Import the downloaded data\n",
    "- (2 pts.) Convert to lowercase\n",
    "- (2 pts.) Remove stopwords \n",
    "- (2 pts.) Remove punctuation and any other non-alphabet characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKbrcXjpPmp8"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import pandas as pd\n",
    "\n",
    "get_ipython().config.get('IPKernelApp', {})['parent_appname'] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00017.txt</td>\n",
       "      <td>THE BOOK OF MORMON\\n\\nAn Account Written\\n\\nBY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00048.txt</td>\n",
       "      <td>THE WORLD FACTBOOK 1992\\n\\n\\n:Afghanistan Geog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00056.txt</td>\n",
       "      <td>When Senator Al Gore was evangelizing support ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00064.txt</td>\n",
       "      <td>[Frontispiece: The cold hollow eye of a revolv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00089.txt</td>\n",
       "      <td>1.   Except as provided in this Annex, each Pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                            content\n",
       "0  00017.txt  THE BOOK OF MORMON\\n\\nAn Account Written\\n\\nBY...\n",
       "1  00048.txt  THE WORLD FACTBOOK 1992\\n\\n\\n:Afghanistan Geog...\n",
       "2  00056.txt  When Senator Al Gore was evangelizing support ...\n",
       "3  00064.txt  [Frontispiece: The cold hollow eye of a revolv...\n",
       "4  00089.txt  1.   Except as provided in this Annex, each Pa..."
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folders = list(range(0, 2))\n",
    "for i, folder in enumerate(folders):\n",
    "    if len(str(folder)) == 1:\n",
    "        folders[i] = '00' + str(folder)\n",
    "    elif len(str(folder)) == 2:\n",
    "        folders[i] = '0' + str(folder)\n",
    "    else:\n",
    "        folders[i] = str(folder)\n",
    "\n",
    "_list = list()\n",
    "for folder in folders:\n",
    "    subfolder = './data/hw5_data/gutenberg-dammit-files/%s' % folder\n",
    "    for file in os.listdir(subfolder):\n",
    "        if re.search('\\d+', file):\n",
    "            filepath = os.path.join(subfolder, file)\n",
    "            content = open(filepath, 'r', encoding = 'utf-8').read()\n",
    "            df = pd.DataFrame(data = {'file': [file], 'content': [content]})\n",
    "            _list.append(df)\n",
    "            \n",
    "raw_data = pd.concat(_list)\n",
    "raw_data.reset_index(drop = True, inplace = True)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00017.txt</td>\n",
       "      <td>the book of mormon\\n\\nan account written\\n\\nby...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00048.txt</td>\n",
       "      <td>the world factbook 1992\\n\\n\\n:afghanistan geog...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00056.txt</td>\n",
       "      <td>when senator al gore was evangelizing support ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00064.txt</td>\n",
       "      <td>[frontispiece: the cold hollow eye of a revolv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00089.txt</td>\n",
       "      <td>1.   except as provided in this annex, each pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                            content\n",
       "0  00017.txt  the book of mormon\\n\\nan account written\\n\\nby...\n",
       "1  00048.txt  the world factbook 1992\\n\\n\\n:afghanistan geog...\n",
       "2  00056.txt  when senator al gore was evangelizing support ...\n",
       "3  00064.txt  [frontispiece: the cold hollow eye of a revolv...\n",
       "4  00089.txt  1.   except as provided in this annex, each pa..."
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_data = raw_data.copy()\n",
    "lower_data.content = lower_data.content.str.lower()\n",
    "lower_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00017.txt</td>\n",
       "      <td>book  mormon\\n\\n account written\\n\\n  hand  m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00048.txt</td>\n",
       "      <td>world factbook 1992\\n\\n\\n:afghanistan geograp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00056.txt</td>\n",
       "      <td>senator al gore  evangelizing support   visio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00064.txt</td>\n",
       "      <td>[frontispiece:  cold hollow eye   revolver sou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00089.txt</td>\n",
       "      <td>1.     provided   annex,  party shall apply\\n ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                            content\n",
       "0  00017.txt   book  mormon\\n\\n account written\\n\\n  hand  m...\n",
       "1  00048.txt   world factbook 1992\\n\\n\\n:afghanistan geograp...\n",
       "2  00056.txt   senator al gore  evangelizing support   visio...\n",
       "3  00064.txt  [frontispiece:  cold hollow eye   revolver sou...\n",
       "4  00089.txt  1.     provided   annex,  party shall apply\\n ..."
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(r'C:\\Users\\dteng\\Anaconda\\Lib\\site-packages\\en_core_web_sm\\en_core_web_sm-2.0.0')\n",
    "lower_nostop_data = lower_data.copy()\n",
    "pat = r'\\b(?:{})\\b'.format('|'.join(list(STOP_WORDS)))\n",
    "lower_nostop_data.content = lower_nostop_data.content.str.replace(pat, '')\n",
    "lower_nostop_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file</th>\n",
       "      <th>content</th>\n",
       "      <th>paragraphs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00017.txt</td>\n",
       "      <td>book  mormon\\n\\n account written\\n\\n  hand  m...</td>\n",
       "      <td>[ book  mormon,  account written,   hand  morm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00048.txt</td>\n",
       "      <td>world factbook \\n\\n\\nafghanistan geography\\n\\...</td>\n",
       "      <td>[ world factbook , afghanistan geography, tota...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00056.txt</td>\n",
       "      <td>senator al gore  evangelizing support   visio...</td>\n",
       "      <td>[ senator al gore  evangelizing support   visi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00064.txt</td>\n",
       "      <td>frontispiece  cold hollow eye   revolver sough...</td>\n",
       "      <td>[frontispiece  cold hollow eye   revolver soug...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00089.txt</td>\n",
       "      <td>provided   annex  party shall apply\\n agr...</td>\n",
       "      <td>[     provided   annex  party shall apply,  ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        file                                            content  \\\n",
       "0  00017.txt   book  mormon\\n\\n account written\\n\\n  hand  m...   \n",
       "1  00048.txt   world factbook \\n\\n\\nafghanistan geography\\n\\...   \n",
       "2  00056.txt   senator al gore  evangelizing support   visio...   \n",
       "3  00064.txt  frontispiece  cold hollow eye   revolver sough...   \n",
       "4  00089.txt       provided   annex  party shall apply\\n agr...   \n",
       "\n",
       "                                          paragraphs  \n",
       "0  [ book  mormon,  account written,   hand  morm...  \n",
       "1  [ world factbook , afghanistan geography, tota...  \n",
       "2  [ senator al gore  evangelizing support   visi...  \n",
       "3  [frontispiece  cold hollow eye   revolver soug...  \n",
       "4  [     provided   annex  party shall apply,  ag...  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_nostop_alpha_data = lower_nostop_data.copy()\n",
    "pat = r'[\\d!?.,\\[\\]/@#$%^&*():;]'\n",
    "lower_nostop_alpha_data.content = lower_nostop_alpha_data.content.str.replace(pat, '')\n",
    "lower_nostop_alpha_data['paragraphs'] = lower_nostop_alpha_data.content.str.split('\\n')\n",
    "\n",
    "def remove_blanks(row):\n",
    "    cleaned = []\n",
    "    for item in row['paragraphs']:\n",
    "        if item != '':\n",
    "            cleaned.append(item)\n",
    "            \n",
    "    return cleaned\n",
    "\n",
    "lower_nostop_alpha_data.paragraphs = lower_nostop_alpha_data.apply(remove_blanks, axis = 1)\n",
    "lower_nostop_alpha_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34ffTIJnPnG2"
   },
   "source": [
    "## Q2. (10 pts.) Frequency of Part of Speech (POS) tags\n",
    "- Use spaCy\n",
    "  - Use the original (or uncleaned) documents. spaCy does POS tagging better when the sentence is more complete. \n",
    "- (4 pts.) Extract POS tags from the first 100 sentences of selected 5 books. \n",
    "- (4 pts.) Plot the frequency of POS tags\n",
    "- (2 pts.) Provide your interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_FjAfpIPuSJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGgcRiT6Pumu"
   },
   "source": [
    "## Q3. (10 pts.) Make wordclouds for 5 books\n",
    "- Use the [```word_cloud```](https://github.com/amueller/word_cloud) package.\n",
    "- (8 pts.) Create a wordcloud for 5 books that you choose.\n",
    "  - The author of each book should be different.\n",
    "- (2pts.) Provide your interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Poomb244P-93"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcnTeSbcQAV7"
   },
   "source": [
    "## Q4. (bonus 5 pts.) Train your own word2vec vector.\n",
    "- Use the ```gensim``` package\n",
    "  - Your input will be the output from Q1 (cleaned data)\n",
    "- Hint: You can look into this tutorial for training the new Word2Vec model: https://rare-technologies.com/word2vec-tutorial/\n",
    "- Use default parameters when you train the Word2Vec model (e.g., ```min_count```, ```size```, etc.), except the number of workers (```workers=?```) parameter.\n",
    "- Use this model for the later questions. \n",
    "  - **If you do not have your Word2Vec model, please use the pre-trained model that we used for the lab to answer the later questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHs8CEEiQo3i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nszpiA0UQsc8"
   },
   "source": [
    "## Q5. (15 pts.) Similarity from Word2Vec \n",
    "- Choose 5 words\n",
    "- (5pts.) What are the top-10 similar words of each?\n",
    "- (5pts.) Do you find anything odd or interesting? Please explanation. \n",
    "  - (if you have your own word2vec model)How the results are different from the pre-trained model that we used in the lab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1iMSir9RJCv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDRW7KNvRJ9Y"
   },
   "source": [
    "## Q6. (20 pts.) Similarity between chapters \n",
    "- (5 pts.) Pick 5 chapters from the dataset and describe.\n",
    "  - Each \"chapter\" consists of 10 consecutive paragraphs from a single book.\n",
    "  - The author of each chapter should be different.\n",
    "  - If necessary, explain how you defined \"paragraph\". \n",
    "- (5 pts.) Calculate the average of vectors for the words included in a chapter.  \n",
    "  - In this way, you can have a vector that can roughly summarize the contents.    \n",
    "  - The average vector should have the same dimensionality with word vectors \n",
    "    - e.g., if the word vector is a 100 dimension, the average vector for paragraph should be a 100 dimension as well\n",
    "  - Words should come from the results of Q1 (cleaned out data) (i.e. should not contain stopwords or other special characters).\n",
    "- (5 pts.) Calculate the similarity between chapters.  \n",
    "  - Provide your interpretation. \n",
    "  - Why do you think some chapters are similar/dissimilar to others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-5G15xTSokY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glHArDFoWtdf"
   },
   "source": [
    "## Q7. (15 pts.) Word clustering\n",
    "- Pick top-100 similar words with the word *information*.  \n",
    "- Calculate cosine similarity scores between words\n",
    "  - e.g., Create a 100x100 matrix that contains cosine similarity scores. Each row and columns should be words. \n",
    "- Use seaborn's [```.clustermap()```](https://seaborn.pydata.org/generated/seaborn.clustermap.html) function to draw a hierarchically-clustered heatmap\n",
    "- Provide your interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-1tqXGVh4q2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "618_HW4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
