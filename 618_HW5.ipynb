{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUfEjrnxfqbP"
   },
   "source": [
    "# SI 618 Homework 5: Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HiF5779SXEsY"
   },
   "source": [
    "## The total score for this assignment will be 100 points, consisting of:\n",
    "- 10 pt: Overall quality of spelling, grammar, puctuation, etc. of written sentences.\n",
    "- 10 pt: Codes are written in [PEP 8](https://www.python.org/dev/peps/pep-0008/) style.\n",
    "- 80 pt: Homework questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Ri4qlLSPL5W"
   },
   "outputs": [],
   "source": [
    "MY_UNIQNAME = '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HZAtMMYfPdPJ"
   },
   "source": [
    "# Download the data:\n",
    "We will be using a subset of the data from [Project Gutenberg](http://www.gutenberg.org).\n",
    "\n",
    ">Project Gutenberg is a volunteer effort to digitize and archive cultural works, to \"encourage the creation and distribution of eBooks\". It was founded in 1971 by American writer Michael S. Hart and is the oldest digital library. Most of the items in its collection are the full texts of public domain books. The project tries to make these as free as possible, in long-lasting, open formats that can be used on almost any computer., Project Gutenberg reached 57,000 items in its collection of free eBooks.\n",
    "\n",
    "We will use a [cleaned up corpus](https://github.com/aparrish/gutenberg-dammit) that was processed by [Allison Parish](https://www.decontextualize.com/). This processing step was done to standardize metadata and text encoding. The dataset is described on the [project page](https://github.com/aparrish/gutenberg-dammit#gutenberg-dammit). Note that there are plain text files withing subdirectories numbered with the first three digits of the document ID, and there is also a JSON file of document metadata with title, author, and other information.\n",
    "\n",
    "\n",
    "We created a reduced subset of 4000 documents for the purpose of this assignment that can be downloaded below:\n",
    "\n",
    "https://drive.google.com/file/d/1V5ep_5veAccCD-YxGlVfv3DsXczBIvv5/view?usp=sharing\n",
    "\n",
    "This ZIP file is approximately 550 MB compressed, and 1.45 GB uncompressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vmyEphdRPjQT"
   },
   "source": [
    "## Q1. (10 pts.) Data cleaning\n",
    "- (3 pts.) Import the downloaded data\n",
    "- (2 pts.) Convert to lowercase\n",
    "- (2 pts.) Remove stopwords \n",
    "- (2 pts.) Remove punctuation and any other non-alphabet characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XKbrcXjpPmp8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34ffTIJnPnG2"
   },
   "source": [
    "## Q2. (10 pts.) Frequency of Part of Speech (POS) tags\n",
    "- Use spaCy\n",
    "  - Use the original (or uncleaned) documents. spaCy does POS tagging better when the sentence is more complete. \n",
    "- (4 pts.) Extract POS tags from the first 100 sentences of selected 5 books. \n",
    "- (4 pts.) Plot the frequency of POS tags\n",
    "- (2 pts.) Provide your interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P_FjAfpIPuSJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sGgcRiT6Pumu"
   },
   "source": [
    "## Q3. (10 pts.) Make wordclouds for 5 books\n",
    "- Use the [```word_cloud```](https://github.com/amueller/word_cloud) package.\n",
    "- (8 pts.) Create a wordcloud for 5 books that you choose.\n",
    "  - The author of each book should be different.\n",
    "- (2pts.) Provide your interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Poomb244P-93"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jcnTeSbcQAV7"
   },
   "source": [
    "## Q4. (bonus 5 pts.) Train your own word2vec vector.\n",
    "- Use the ```gensim``` package\n",
    "  - Your input will be the output from Q1 (cleaned data)\n",
    "- Hint: You can look into this tutorial for training the new Word2Vec model: https://rare-technologies.com/word2vec-tutorial/\n",
    "- Use default parameters when you train the Word2Vec model (e.g., ```min_count```, ```size```, etc.), except the number of workers (```workers=?```) parameter.\n",
    "- Use this model for the later questions. \n",
    "  - **If you do not have your Word2Vec model, please use the pre-trained model that we used for the lab to answer the later questions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHs8CEEiQo3i"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nszpiA0UQsc8"
   },
   "source": [
    "## Q5. (15 pts.) Similarity from Word2Vec \n",
    "- Choose 5 words\n",
    "- (5pts.) What are the top-10 similar words of each?\n",
    "- (5pts.) Do you find anything odd or interesting? Please explanation. \n",
    "  - (if you have your own word2vec model)How the results are different from the pre-trained model that we used in the lab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c1iMSir9RJCv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TDRW7KNvRJ9Y"
   },
   "source": [
    "## Q6. (20 pts.) Similarity between chapters \n",
    "- (5 pts.) Pick 5 chapters from the dataset and describe.\n",
    "  - Each \"chapter\" consists of 10 consecutive paragraphs from a single book.\n",
    "  - The author of each chapter should be different.\n",
    "  - If necessary, explain how you defined \"paragraph\". \n",
    "- (5 pts.) Calculate the average of vectors for the words included in a chapter.  \n",
    "  - In this way, you can have a vector that can roughly summarize the contents.    \n",
    "  - The average vector should have the same dimensionality with word vectors \n",
    "    - e.g., if the word vector is a 100 dimension, the average vector for paragraph should be a 100 dimension as well\n",
    "  - Words should come from the results of Q1 (cleaned out data) (i.e. should not contain stopwords or other special characters).\n",
    "- (5 pts.) Calculate the similarity between chapters.  \n",
    "  - Provide your interpretation. \n",
    "  - Why do you think some chapters are similar/dissimilar to others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-5G15xTSokY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "glHArDFoWtdf"
   },
   "source": [
    "## Q7. (15 pts.) Word clustering\n",
    "- Pick top-100 similar words with the word *information*.  \n",
    "- Calculate cosine similarity scores between words\n",
    "  - e.g., Create a 100x100 matrix that contains cosine similarity scores. Each row and columns should be words. \n",
    "- Use seaborn's [```.clustermap()```](https://seaborn.pydata.org/generated/seaborn.clustermap.html) function to draw a hierarchically-clustered heatmap\n",
    "- Provide your interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p-1tqXGVh4q2"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "618_HW4.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
